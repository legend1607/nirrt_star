III. METHOD A. Problem Definition We define the optimal path planning problem similar to related works [4]–[6]. The state space is denoted as X ⊆ Rd. The obstacle space and the free space are denoted as Xobs and Xfree. A path σ : [0, 1] → Xfree is a sequence of states. The set of paths is denoted as Σ. The optimal path planning problem is to find a path σ∗ which minimizes a given cost function c : Σ → R≥0, connects a given start state xstart ∈ Xfree and a given goal state xgoal ∈ Xfree, and has all states on the path in free space. Formally: σ∗ = arg min σ∈Σ c (σ) s.t. σ(0) = xstart, σ(1) =xgoal, ∀s ∈ [0, 1], σ(s) ∈ Xfree (1) B. Neural Informed RRT* We present NIRRT* in Algorithm 1, where the unhighlighted part is from RRT*, the blue part is from IRRT*, and the red part is our contribution. We track the best path solution cost ci best through each iteration, which is initialized as infinity (line 3). We initialize update cost cupdate with the value of c0 best (line 4). We call the neural network to infer an initial guidance state set Xguide based on the complete free state space (line 5). As better solutions are found, the guidance state set Xguide may be updated by the neural network calls depending on how much the path cost has been improved, and random samples xrand are sampled using both Xguide and informed sampling (line 8). PointNetGuidedSampling: When the current best path cost ccurr is less than the path cost improvement ratio α ≤ 1 of cupdate, the neural network is called to update Xguide, and cupdate is updated by ccurr. The random sample xrand is sampled with a mixed strategy: if a random number Rand() ∈ (0, 1) is smaller than 0.5, we use the sampling strategy of IRRT* to sample xrand; otherwise, we sample xrand uniformly from Xguide. Similar to [6], [15], [18], our mixed sampling strategy guarantees probabilistic completeness and asymptotic optimality by implementing the sampling procedure of IRRT* with a non-zero probability. Note the frequency of calling neural networks for guidance state inference is controlled by the path cost improvement ratio α. If we do not update Xguide after initial inference, and remove IRRT* components, NIRRT* is reduced to NRRT*. While NIRRT* is generic in that any neural network that infers guidance states can fit into the framework, we emphasize the use of a point-based network. In the next subsection, we discuss the details of Point-based Network Guidance (PNG), and explain the preference of point representations over grid representations. C. Point-based Network Guidance Point-based Network. We represent the state space by a point cloud Xinput = {x1, x2, . . . , xN } ⊂ Xfree. The density of point cloud should allow a reasonable amount of neighbors around each point in radius of step size η. We oversample points uniformly from Xfree, and perform minimum distance downsampling to obtain the point cloud with even distribution. We create a one-hot vector for each point, indicating whether the point is within radius η of xstart or xgoal. We concatenate the one-hot vectors with normalized point coordinates to generate point cloud representations of the free states. The processed point cloud  ̄ Xinput is fed into a point-based network f . The network f maps each point to a probability pi ∈ [0, 1], where the points with probability greater than 0.5 form the set of guidance states Xguide. Formally, {p1, p2, . . . , pN } = f (  ̄ Xinput), Xguide = {xi|pi > 0.5}. (2) We implement PointNet++ [20] as the model architecture of the point-based network. Since PointNet++ is originally designed for 3D point cloud, we set z coordinates as zero for 2D problems. We collect 4,000 2D random worlds as the training dataset. For each random world, we run A* in pixel space with step size of unit pixel and clearance of 3 pixels to generate the pixel-wise optimal path. We generate a point cloud of number N = 2048, and generate guidance state labels by checking whether each point is around any point of the pixel-wise optimal path in radius of η, which is set as 10 pixels. We train PointNet++ by Adam optimizer [31] with an initial learning rate of 0.001 and batch size of 16 for 100 epochs. We use the trained model across all types of 2D planning problems. For 3D random world problems, we follow a similar scheme, but the clearance is set as 2 voxels. Neural Focus. Informed RRT* outperforms RRT* by proposing a heuristic ellipsoidal subset of the planning domain Xfocus in terms of the current best solution cost ccurr, in order to sample xrand which is more likely to improve the current solution. The reasoning behind this sampling strategy is that for any state xrejected from X\Xfocus, the minimum cost of a feasible path from xstart to xgoal through xrejected is greater than ccurr: Xfocus = {x ∈ X  ||x − xstart||2 + ||x − xgoal||2 ≤ ccurr} (3) Neural Focus is to constrain the point cloud input to the point-based network inside the Xfocus, which is equivalent as changing the domain of oversampling from Xfree to Xfocus ∩ Xfree. Since we normalize point coordinates when processing point cloud inputs, the trained point-based network can handle point clouds sampled from domains at different scales. With the same number of points N , a smaller volume of Xfocus leads to a denser point cloud, which describes important regions with finer details. For example, Figure 3(b) shows that Neural Focus fills the narrow passage with a large number of points, which is captured by the point-based network to produce more effective inference on guidance states compared to Figure 3(a). Neural Connect. The points close to xstart or xgoal are usually classified as guidance states with greater probabilities than the points around midway of the path (e.g., Figure 3(d)). When the distance between xstart and xgoal gets longer, the guidance state set Xguide is more likely to be separated into disconnected “blobs”. This phenomenon of probability polarization is reported in NRRT* work [6]. Our experiments show lack of connectivity limits the performance in large and complex planning problems. We address this issue by introducing Neural Connect, which is inspired by RRT-Connect [21]. We initialize Xguide as an empty set, xs1tart as xstart, and x1 goal as xgoal. During iteration, we first call the point-based network with xsitart and xi goal as start and goal, and add inferred guidance states to Xguide. Second, We run Breadth First Search (BFS) from xstart to xgoal through the guidance states in Xguide. The neighbor radius of BFS is set as η, and no collision check is performed. After BFS is finished, connectivity of Xguide is confirmed if xgoal is reached. Otherwise, we find the boundary points Xbound of the states visited by BFS by checking whether any points in Xinput\Xguide are around the visited state of radius η/2. We select xi+1 start from Xbound which is one of the states heuristically the furthest from xstart and one of the states to reach xgoal with minimum total heuristic cost. Third, we perform the same operation as the second step, with the start of BFS as xgoal, and the goal of BFS as xstart. We obtain xi+1 goal if connectivity is negative. We perform the iteration until connectivity is built or the limit of iteration nguide is reached, which we set as 5 in practice. We illustrate Neural Connect in Figure 3(c-h). Note the orange path found by BFS in Figure 3(h) does not go through collision check, so the path is not a feasible solution but a visual demonstration on the connectivity of Xguide. PointNetGuide: We apply both Neural Focus and Neural Connect to the point-based network, and obtain the complete module of Point-based Network Guidance, which is presented in Algorithm 3.